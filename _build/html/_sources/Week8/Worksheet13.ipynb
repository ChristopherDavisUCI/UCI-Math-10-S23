{"cells":[{"cell_type":"markdown","source":"# Worksheet 13\n\nThe goal of this worksheet is to use a Decision Tree classifier to predict whether or not a passenger of the Titanic survived.\n\nMany of the ideas in this worksheet come from [this notebook](https://www.kaggle.com/code/zlatankr/titanic-random-forest-82-78) on Kaggle by [ZlatanKremonic](https://www.kaggle.com/zlatankr).  The dataset we use comes from a [Kaggle competition](https://www.kaggle.com/competitions/titanic).","metadata":{"tags":[],"cell_id":"67d9233243304581bd192c01a94f8ef8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Feature Engineering\n\n* Load the attached Titanic dataset.\n* Using Boolean indexing, remove the rows where the \"Embarked\" column value is missing.  (This should only remove two rows.)\n* Drop the \"PassengerId\" column using the `drop` method, with `drop(\"PassengerId\", axis=???)`.  You should probably use the `copy` method to prevent warnings in the next step.\n* Add a column \"AgeNull\" which contains `True` if the value in the \"Age\" column is missing and contains `False` otherwise.  The code to do this is shorter than you might expect: `df[\"AgeNull\"] = df[\"Age\"].isna()`.\n* Fill in the missing values in the \"Age\" column with the median value from that column. Use the pandas Series method `fillna`.  (Replace the \"Age\" column with this new column that does not have any missing values.)\n* Add a column \"IsFemale\" which contains `True` if the value in the \"Sex\" column is `\"female\"`.  (As with the \"AgeNull\" column above, you shouldn't need to use `map` or a for loop or anything like that.)\n* Add a column \"CabinLetter\" which contains the first character in the string from the \"Cabin\" column.  Use the pandas Series method `map`, together with a lambda function and the `na_action` keyword argument so that missing values don't raise an error.  (Look up the documentation for `map` to see what the possible values are for `na_action`.)\n* Check your answer.  The current DataFrame should have 889 rows and 14 columns.","metadata":{"tags":[],"cell_id":"ff466151234345b5acbe34cb2173095d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## One-hot encoding\n\n* Use scikit-learn's `OneHotEncoding` class to perform one-hot encoding on the columns \"Embarked\" and \"CabinLetter\".  (Use both columns at once, for example, `encoder.fit(df[[\"Embarked\", \"CabinLetter\"]])`.)\n* Include the transformed columns in the DataFrame. Use `encoder.get_feature_names_out()` to get the names of these columns.\n* Check your answer.  The current DataFrame should still have 889 rows, but now it should have 26 columns.","metadata":{"tags":[],"cell_id":"8f36b312ecdf437d86c0446458c74add","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Splitting the data with `train_test_split`","metadata":{"tags":[],"cell_id":"97f6bb0dc7034360a31aa6e5b972c511","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"* Make a list `features` containing the names of all the numeric columns in the DataFrame except for the \"Survived\" column.  (Use list comprehension together with `is_numeric_dtype` from `pandas.api.types`.  You can do this all at once, or first make a list and then get rid of the \"Survived\" entry using the Python list method `remove`.  Notice that `remove` changes this list *in place*.)\n* Divide the data into a training set and a test set using `train_test_split`.  For the input features, use `df[features]`.  For the target, use the \"Survived\" column.  For the size, use `train_size` to specify that we should use 60% of the rows for the training set.  Name the resulting objects `X_train, X_test, y_train, y_test`.","metadata":{"tags":[],"cell_id":"48884af9e4854cd180d0157a35923ede","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Predicting survival using a decision tree\n\n* Instantiate a `DecisionTreeClassifier` object `clf`.  Include restrictions on the complexity of the tree using the keyword arguments `max_leaf_nodes` and/or `max_depth` when you instantiate the classifier.  (There is more information on what we are looking for a few steps below.)\n* Fit the classifier using `X_train` and `y_train`.\n* Try to experiment with different values of `max_leaf_nodes` and/or `max_depth` until you have a tree which seems to be performing well (say, over 80% accuracy on the test set, as calculated using `clf.score`) and which does not seem to be drastically overfitting the data (say, the accuracy on the training set should be within 5% of the accuracy on the test set).  Be sure you are never calling the `fit` method with the test set; you should only use the `predict` method or the `score` method with the test set.\n\n**Comment**.  We will later see a more refined way to detect overfitting, using `log_loss` instead of `score`.\n\n* When you have values of `max_leaf_nodes` and/or `max_depth` that seem to be working well, try running the code again, beginning with the `train_test_split` step.  This will give new training and testing data.  The new tree should continue to work well.\n\n* Create a pandas Series using the classifier's `feature_importances_` attribute as the Series values, and using the `feature_names_in_` attribute as the Series index.  Sort the values of this pandas Series from largest to smallest using the `sort_values` method and the `ascending` keyword argument.\n\n* Which features seem to be the most relevant to predicting survival?\n\n**Comment**.  Honestly the one-hot encoded columns do not seem very useful in the examples I have tried, so don't be surprised if most or all of those show up with an importance of 0.","metadata":{"tags":[],"cell_id":"ca8f08557044422f87920a59e029b887","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Saving the results\n\n* When you have a classifier that is working well (i.e., is getting good results on the test set and not severely overfitting the training data), save the resulting `DecisionTreeClassifier` object `clf` as a pickle file named *wkst13-ans.pickle*, as in the [Worksheet 1 instructions](https://christopherdavisuci.github.io/UCI-Math-10-F22/Week1/Worksheet1.html), and upload that pickle file to Canvas.\n* If you want to check that your object saved correctly, you can use the following code.  Then make sure the resulting `clf_saved` object seems correct (for example, it should perform well on the test data).\n\n```\nwith open(\"wkst13-ans.pickle\", \"rb\") as f:\n    clf_saved = pickle.load(f)\n```","metadata":{"tags":[],"cell_id":"5a070e5f1f3445fa9865f919967b0acd","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Reminder\n\nEvery group member needs to submit this on Canvas (even if you all submit the same file).","metadata":{"tags":[],"cell_id":"2d955ad0cfa94ba9a889dda5c743bb2b","source_hash":"733d6982","execution_start":1668277351655,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Submission\n\nSubmit the pickle file on Canvas, as described in the instructions above.","metadata":{"tags":[],"cell_id":"ebf5ab69214947bda3158c5dd034cbbb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6a9b6b01-2155-466e-a7cc-e2f87238cef0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"373b508854904d90b7536261b6edb0a4","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2022-11-12T19:31:07.390Z"}}}