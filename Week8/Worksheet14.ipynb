{"cells":[{"cell_type":"markdown","source":"# Worksheet 14\n\nAuthors (3 maximum; use your full names): **BLANK**\n\nIn this worksheet, we will investigate the [King County dataset](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction) which contains sale prices for houses sold in King County in Washington State.  (Here are the [column definitions](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction/discussion/207885) for this dataset.)\n\nThis house price data contains houses which are much more expensive than typical.  These outliers can have a big impact on our Machine Learning models.","metadata":{"tags":[],"cell_id":"f09cb20d23fb463fa6428c7a9666f065","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Importing the data\n\n* Import the attached file *kc_house_data.csv* as `df`.\n* Define the sub-DataFrame `dfX` as consisting of every row in `df` and consisting of the columns from \"bedrooms\" to the end.  Use `dfX = df.loc[???, ???]` with two slices.\n* Define `sery` as the \"price\" column from `df`.  (We call it `sery` instead of `dfy` because this is a pandas Series.)","metadata":{"tags":[],"cell_id":"ccff4df5f3904ebc83af9c8c564fcd86","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## This data is not normally distributed\n\n(I don't know much about statistics, so please let me know if anything here is wrong!)\n\nMany quantities approximately follow a normal distribution, but house prices definitely do not.  Let's briefly see that in the context of this dataset.\n\n* How many total houses are there in this dataset?\n* What is the mean price value?\n* What is the standard deviation of the price values?\n* What is the maximum house price in this dataset?\n* How many houses in this data set have a price over three million dollars?  (First make a Boolean Series containing `True` if the house price is greater than three million and `False` otherwise, and then call the `sum` method on this Boolean Series.)\n* In a normal distribution with this mean and this standard deviation, what percentage of houses what percentage of houses would have value under three million?  First import the `norm` object from `scipy.stats`, and then use the following code.  (The `:.15%` tells Python to display the number as a percentage, not as a probability, and to include 15 decimal places.)\n```\nf\"{norm.cdf(3000000, loc=???, scale=???):.15%}\"\n```\n","metadata":{"tags":[],"cell_id":"9676f1aee6bc4e62ad6221c99fe9789b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Price predictions using a Decision Tree with Mean Absolute Error\n\nCreate an instance of either `DecisionTreeRegressor` or `DecisionTreeClassifier` from `sklearn.tree`.  (Which makes sense for this supervised learning task?)  Name the object `reg` or `clf`, as appropriate.  Specify the following keyword arguments when you instantiate this object.\n* Set `criterion=\"???\"`.  Look up the options, and choose the option corresponding to Mean Absolute Error.\n* Set `max_depth=3`.\n* Set `max_leaf_nodes=5`.","metadata":{"tags":[],"cell_id":"f2eaa545baf34230949ed756417c5d60","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"* Fit the object using `dfX` as the input features and using `sery` as the target.  (This took about two minutes when I tried.  It's not obvious why, but the fitting will be much faster below when we use Mean Squared Error instead.)","metadata":{"tags":[],"cell_id":"a2cca73fca244d809473a0d5d94cba2c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"* Make a diagram illustrating the fit tree, using the following code.\n\n```\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nfig = plt.figure(figsize=(10,10))\n_ = plot_tree(\n    reg,\n    feature_names=reg.feature_names_in_,\n    filled=True\n)\n```","metadata":{"tags":[],"cell_id":"e3b22a8344cf4cc0a6597cdc35d3df7c","source_hash":"78914f20","execution_start":1667960828770,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Refer to the above tree diagram when answering the following questions in a markdown cell (not in a Python comment).\n* Which features (columns) were used by this tree?\n* Do you see how `max_depth=3` is reflected in this diagram?  What about `max_leaf_nodes=5`?","metadata":{"tags":[],"cell_id":"9dc7fccbb27b4bbd8fab8cd1ee207ac5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Another tree diagram, using Mean Squared Error\n\n* Make a new instance, this time using Mean Squared Error as the `criterion`, but keeping the other parameters the same.\n* Again fit the object.  (The computation should be much faster than in the example using Mean Absolute Error.  I don't know why that is.)\n* Again make a tree diagram.","metadata":{"tags":[],"cell_id":"c1f3c641abd241c9adff968516d65b35","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Answer the following in a markdown cell (not as a Python comment).\n* Notice how one of the leaf nodes contains very few samples.  Was the same true in the previous diagram?\n* Which tends to be more influenced by outliers, Mean Squared Error or Mean Absolute Error?  How does that relate to the previous question?","metadata":{"tags":[],"cell_id":"c1e5b6f1a590484a86fc86b0a0b2f6e4","source_hash":"23371beb","execution_start":1668537881141,"execution_millis":602,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Dividing the data\n\nWe want to illustrate how decision trees are prone to overfitting.  This will be more obvious if we use less than the full 20,000 rows in the dataset, and if we don't use the most relevant columns.  (For example, instead of using the size of the house, we will use the size of the basement.)\n\n* Call the `sample` method of `df` to select 2000 random rows.  Specify `random_state=34`.  (This value was chosen to make sure some of the most expensive houses are included in the sample.)  Name this 2000 row DataFrame `df2`.\n* Define `sery2` to be the \"price\" column from `df2`, and define `dfX2` to be the following columns from `df2`: \"sqft_basement\", \"zipcode\", \"yr_built\", \"condition\", \"view\", \"waterfront\", \"condition\".  (We are choosing these columns because they are somewhat random and thus do a good job of illustrating overfitting.)\n* Divide these into a training set and a test set using the following:\n```\nX_train, X_test, y_train, y_test = train_test_split(\n    dfX2,\n    sery2,\n    train_size=0.8,\n    random_state=4\n)\n``` \n(We are setting `random_state=4` because that value will make a nice U-shaped test error curve below.  Other values I tried also make a U-shaped test error curve, but `4` is the value I found where the U-shape is most clear.)","metadata":{"tags":[],"cell_id":"c4dcdc16d98e4be7a10355d9cd30a946","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Training errors and Test errors\n\n* Define two empty dictionaries, `train_dict` and `test_dict`.  (To define an empty dictionary, you can use `{}`.)\n\nFor each integer value of `n` from `2` to `300`, do the following.  (In other words, each step will be repeated 299 times.  Use a for loop, and put all of the following inside the body of the for loop.  The entire for loop should only take about 10 seconds to run.  This is quite a contrast to the situation above, when we were using Mean Absolute Error.)\n* Instantiate a new `DecisionTreeRegressor` using Mean Squared Error for the `criterion`, using `max_depth=50`, and using `max_leaf_nodes` as `n`.\n* Fit the regressor using `X_train` and `y_train`.\n* Using `mean_squared_error` from `sklearn.metrics`, compute the Mean Squared Error between `y_train` and the predicted values.  Put this error as a value into the `train_dict` dictionary with the key `n`.\n* Using `mean_squared_error` from `sklearn.metrics`, compute the Mean Squared Error between `y_test` and the predicted values.  (Be sure you do not fit the regressor again!  We should never fit a model using the test set.)  Put this error as a value into the `test_dict` dictionary with the key `n`.","metadata":{"tags":[],"cell_id":"e45234ad932a4e5996c764b3fc88135c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## U-shaped test error curve\n\nUse the following code to show the corresponding training loss and test loss.  We want the training loss curve to be colored differently from the test loss curve.  The result should reflect the U-shaped test error curve that often occurs in situations of overfitting.\n\n```\ntrain_ser = pd.Series(train_dict)\ntest_ser = pd.Series(test_dict)\ntrain_ser.name = \"train\"\ntest_ser.name = \"test\"\ndf_loss = pd.concat((train_ser, test_ser), axis=1)\ndf_loss.reset_index(inplace=True)\ndf_loss.rename({\"index\": \"max_leaf_nodes\"}, axis=1, inplace=True)\ndf_melted = df_loss.melt(id_vars=\"max_leaf_nodes\", var_name=\"Type\", value_name=\"Loss\")\n```\nfollowed by\n```\nalt.Chart(df_melted).mark_line().encode(\n    x=???, # bigger values = more flexible\n    y=???, # bigger values = worse performance\n    color=??? # The train curve in one color, the test curve in another.\n)\n```\n\nHint.  To help you make the chart, try evaluating `df_melted` to see what column names it has.","metadata":{"tags":[],"cell_id":"328cffb4b3d54d96a2ce0085a63abac7","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Interpretation questions\n\nAnswer the following in a markdown cell (not as a Python comment).\n\n* Where does the model seem to be underfitting the data?\n* Where does the model seem to be overfitting the data?\n* If you had to use one of these 299 models on new data, which would you choose?  Why?\n* What direction in the chart corresponds to models with \"more flexibility\"?","metadata":{"tags":[],"cell_id":"7f7544d8c96e45aa9aad19ef7b623fb1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Reminder\n\nEvery group member needs to submit this on Canvas (even if you all submit the same link).\n\nBe sure you've included the (full) names of you and your group members at the top after \"Authors\".  You might consider restarting this notebook (the Deepnote refresh icon, not your browser refresh icon) and running it again to make sure it's working.","metadata":{"tags":[],"cell_id":"0654863e1f9f46a88fdc1e9d6269225a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Submission\n\nUsing the Share button at the top right, **enable Comment access level** for anyone with the link. Then submit that link on Canvas.","metadata":{"tags":[],"cell_id":"859ddc27ec7e448795d566423c91477a","source_hash":"e3c520d1","execution_start":1667964919251,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ecc3eaee-8db2-43e4-b77a-b95ebebaa35c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"61cc3e6670fb4687871c00e537a88390","deepnote_execution_queue":[]}}